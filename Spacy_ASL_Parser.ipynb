{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Capstone/ASLParser"
      ],
      "metadata": {
        "id": "Qqpekrgo3dJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVwDgVoq229D"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "import json\n",
        "from spacy.training import Example\n",
        "import re\n",
        "!pip install Levenshtein\n",
        "import Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"EngToASLPairs.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Remove newline characters and filter out empty lines\n",
        "lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "# Group English and ASL lines into pairs\n",
        "for i in range(0, len(lines), 2):\n",
        "    if i + 1 < len(lines):\n",
        "        data.append({\"english\": lines[i], \"asl\": lines[i + 1]})\n"
      ],
      "metadata": {
        "id": "fyxXHz0A3czQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "6dLnUBKjPp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English model\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "k6Ye2mSd3Iad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_asl_token(asl_token_text):\n",
        "    asl_token_text = re.sub(r'[^\\w\\s]', '', asl_token_text)\n",
        "    return asl_token_text"
      ],
      "metadata": {
        "id": "rnHef7Xa2_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_asl_token(english_token_text, asl_tokens_texts, threshold=1):\n",
        "    min_distance = float('inf')\n",
        "    best_match = None\n",
        "\n",
        "    # Lowercase and lemmatize English token\n",
        "    english_lemma = nlp_en(english_token_text.lower())[0].lemma_\n",
        "\n",
        "    for asl_token_text in asl_tokens_texts:\n",
        "        # Lowercase and lemmatize ASL token using English lemmatization rules\n",
        "        asl_lemma = nlp_en(asl_token_text.lower())[0].lemma_\n",
        "\n",
        "        distance = Levenshtein.distance(english_lemma, asl_lemma)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_match = asl_token_text\n",
        "    if min_distance <= threshold:\n",
        "        return best_match\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "RfCZ41fz3KNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse English sentences and create ASL training data\n",
        "asl_training_data = []\n",
        "num_processed_pairs = 0\n",
        "num_pairs = len(data)\n",
        "for pair in data:\n",
        "    english_doc = nlp_en(pair[\"english\"])\n",
        "    # for token in english_doc:\n",
        "    #     print(f\"Token: {token.text}, POS: {token.pos_}, DEP: {token.dep_}\")\n",
        "    asl_tokens_texts = preprocess_asl_token(pair[\"asl\"]).split()\n",
        "\n",
        "    asl_deps = [None] * len(asl_tokens_texts)\n",
        "    asl_pos = [None] * len(asl_tokens_texts)\n",
        "\n",
        "    for token in english_doc:\n",
        "        # Find the corresponding ASL token\n",
        "        asl_token = find_best_matching_asl_token(token.text, asl_tokens_texts)\n",
        "\n",
        "        if asl_token:\n",
        "            asl_token_index = asl_tokens_texts.index(asl_token)\n",
        "            asl_deps[asl_token_index] = token.dep_\n",
        "            asl_pos[asl_token_index] = token.pos_\n",
        "\n",
        "    asl_training_data.append({\n",
        "        \"text\": preprocess_asl_token(pair[\"asl\"]),\n",
        "        \"deps\": asl_deps,\n",
        "        \"pos\": asl_pos\n",
        "    })\n",
        "\n",
        "    num_processed_pairs += 1\n",
        "    if num_processed_pairs % 50 == 0:\n",
        "      percent_complete = num_processed_pairs / num_pairs * 100\n",
        "      print(f\"Processed {num_processed_pairs} out of {num_pairs} pairs ({percent_complete:.2f}% complete)\")\n"
      ],
      "metadata": {
        "id": "SSchhQC73MYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asl_training_data"
      ],
      "metadata": {
        "id": "P2_AYbqU7qD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ASL training data to JSON\n",
        "with open(\"asl_training_data.json\", \"w\") as f:\n",
        "    json.dump(asl_training_data, f)"
      ],
      "metadata": {
        "id": "q5VVLwNE3UAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train parser\n"
      ],
      "metadata": {
        "id": "VLS-puAO742C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "import json"
      ],
      "metadata": {
        "id": "PyNaRC9yIDsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "with open(\"asl_training_data.json\", \"r\") as f:\n",
        "    dataset = json.load(f)"
      ],
      "metadata": {
        "id": "7abGw_u1OMHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "GA-XiLRK720t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "for entry in dataset:\n",
        "    doc = nlp(entry['text'])\n",
        "    # print(entry)\n",
        "    entry['pos'] = [p if p is not None else '-' for p in entry['pos']]\n",
        "    entry['deps'] = [p if p is not None else '-' for p in entry['deps']]\n",
        "    try:\n",
        "      examples.append(Example.from_dict(doc, {'DEP': entry['deps'], 'POS': entry['pos']}))\n",
        "    except:\n",
        "      print(entry)\n",
        "      continue\n"
      ],
      "metadata": {
        "id": "ms6MfiEpINJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(examples)"
      ],
      "metadata": {
        "id": "YQmQyYS9bqnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-lookups-data\n",
        "!python -m spacy download en\n",
        "!python -m spacy lookups download en\n"
      ],
      "metadata": {
        "id": "ZjbiEPQ1aiuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = dataset*3"
      ],
      "metadata": {
        "id": "OJc-P1FQfd9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "config = {\n",
        "    \"min_action_freq\": 5\n",
        "}\n",
        "nlp.add_pipe(\"parser\", config=config)\n",
        "\n",
        "\n",
        "# Define the other pipes to disable during training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"parser\"]\n",
        "\n",
        "# Disable other pipes and begin training\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    # Begin training\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(10):\n",
        "        # Shuffle the training data\n",
        "        random.shuffle(new_dataset)\n",
        "        for data in new_dataset:\n",
        "            data['pos'] = [p if p is not None else '-' for p in data['pos']]\n",
        "            data['deps'] = [p if p is not None else '-' for p in data['deps']]\n",
        "            text = data[\"text\"]\n",
        "            deps = data[\"deps\"]\n",
        "            pos = data[\"pos\"]\n",
        "\n",
        "            # Create a spacy Doc object from the text\n",
        "            doc = nlp.make_doc(text)\n",
        "            # Create an Example object from the Doc and annotations\n",
        "            example = Example.from_dict(doc, {\"deps\": deps, \"pos\": pos})\n",
        "            print(example)\n",
        "            # Update the parser with the Example\n",
        "            nlp.update([example], sgd=optimizer)\n"
      ],
      "metadata": {
        "id": "Jvb5P5B8aBa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples[1]"
      ],
      "metadata": {
        "id": "RvL-f69NiRIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}