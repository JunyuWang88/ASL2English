{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klC9WRbABweQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Capstone/ASLParser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "from spacy.tokens import Doc\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline import DependencyParser\n",
        "from typing import List, Tuple\n",
        "import json\n",
        "import glob\n"
      ],
      "metadata": {
        "id": "XmgzZik2B8lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_and_append_json_files(pattern):\n",
        "    all_data = []\n",
        "\n",
        "    for file_name in glob.glob(pattern):\n",
        "        with open(file_name, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            all_data.extend(data)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def transform_training_data(data):\n",
        "    transformed_data = []\n",
        "\n",
        "    for item in data:\n",
        "        asl_text = item['asl']['text']\n",
        "        asl_heads = item['asl']['heads']\n",
        "        asl_deps = item['asl']['deps']\n",
        "\n",
        "        if '-' in asl_heads:\n",
        "            continue\n",
        "\n",
        "        transformed_data.append((asl_text, {'heads': asl_heads, 'deps': asl_deps}))\n",
        "\n",
        "    return transformed_data\n",
        "\n",
        "\n",
        "# Load the data from the JSON file\n",
        "def load_from_json(filename):\n",
        "    with open(filename, 'r') as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "VnpjitIgCEd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_training_examples(training_data: List[Tuple]) -> List[Example]:\n",
        "    \"\"\" Create list of training examples \"\"\"\n",
        "    examples = []\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    for text, annotations in training_data:\n",
        "        # print(f\"{text} - {annotations}\")\n",
        "        try:\n",
        "            examples.append(Example.from_dict(nlp(text), annotations))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing: {text} - {annotations}\")\n",
        "            continue\n",
        "    print(\"finish\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def save_trained_nlp(nlp, custom_name):\n",
        "    nlp.to_disk(custom_name)\n",
        "\n",
        "\n",
        "def load_trained_nlp(custom_name):\n",
        "    nlp = spacy.load('en_core_web_sm', exclude=[\"parser\"])\n",
        "    parser_nlp = spacy.load(custom_name)\n",
        "    nlp.add_pipe(\"parser\", source=parser_nlp)\n",
        "    return nlp\n"
      ],
      "metadata": {
        "id": "MsXmJtqbCnf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and append all JSON files\n",
        "pattern = \"english_asl_pairs_raw_*_data.json\"\n",
        "all_data = read_and_append_json_files(pattern)\n",
        "\n",
        "# Transform the data\n",
        "TRAINING_DATA = transform_training_data(all_data)\n",
        "TRAINING_DATA = TRAINING_DATA[5:10]"
      ],
      "metadata": {
        "id": "iSVVUvh8CGjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA"
      ],
      "metadata": {
        "id": "xaubxvipCK39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('en')\n",
        "# Create new parser\n",
        "parser = nlp.add_pipe('parser', first=True)\n",
        "for text, annotations in TRAINING_DATA:\n",
        "    for label in annotations['deps']:\n",
        "        if label not in parser.labels:\n",
        "            parser.add_label(label)\n",
        "print(f\"Added labels: {parser.labels}\")\n"
      ],
      "metadata": {
        "id": "hxtqVEA0Ckmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = create_training_examples(TRAINING_DATA)"
      ],
      "metadata": {
        "id": "osM2GvOYCqal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.initialize(lambda: examples)\n",
        "print(f\"Training ... \", end='')\n",
        "for i in range(25):\n",
        "    print(f\"{i} \", end='')\n",
        "    random.shuffle(examples)\n",
        "    nlp.update(examples, sgd=optimizer)\n",
        "print(f\"... DONE\")\n",
        "\n",
        "save_trained_nlp(nlp, \"new_parser\")"
      ],
      "metadata": {
        "id": "UR29zjzDCsmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}